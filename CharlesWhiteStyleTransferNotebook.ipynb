{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer - AI Essentials werkstuk - Charles D. White"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the process of taking an image and reproducing it with a new artistic style using a pre-trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bronnen:\n",
    "1. https: // arxiv .org/abs/1508.06576 - Leon A. Gatys, Alexander S. Ecker, Matthias Bethge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Style Transfer using the VGG19 Pre-Trained Model in PyTorch in order to calculate the content loss and the style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the style image\n",
    "#Joaquim Falco Dali Portreto Picasso:\n",
    "# https://auction.catawiki.com/kavels/28497261-falco-dali-contemplates-picasso-paintings\n",
    "# Picasso satitirised his Sitters:\n",
    "# https://www.apollo-magazine.com/picassos-portraits-satirised-his-sitters-but-also-art-itself/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries required\n",
    "# basic data science and visualization libraries:\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# imports for the PyTorch libraries utilized:\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tf\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model used for Style Transfer is VGG19 --> True => model comes initialized with pre-trained weights\n",
    "# weights from pre-trained model applied need to remain frozen when performing neural style transfer on images --> False\n",
    "# weights of model do not change; weights of target image will be trained content from content image and style from style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads model weights and instantiates the VGG19 pretrained model with those weights\n",
    "# then, we train target image for neural style transfer using a cuda device when possible\n",
    "# this displays on the screen all of the layers of the VGG19 model stacked up sequentially\n",
    "# every layer has a unique index starting at '0'\n",
    "# there are convolutional layers, ReLU activations and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All pre-trained models in PyTorch expect input image pixels to be in [0,1] range normalized \n",
    "# using the mean and standard deviation, source: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
